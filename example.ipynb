{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabM\n",
    "\n",
    "This is a standalone usage example for the TabM project.\n",
    "The easiest way to run it is [Pixi](https://pixi.sh/latest/#installation):\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/yandex-research/tabm\n",
    "cd tabm\n",
    "\n",
    "# With GPU:\n",
    "pixi run -e cuda jupyter-lab example.ipynb\n",
    "\n",
    "# Without GPU:\n",
    "pixi run jupyter-lab example.ipynb\n",
    "```\n",
    "\n",
    "For the full overview of the project, and for non-Pixi environment setups, see README in the repository:\n",
    "https://github.com/yandex-research/tabm\n",
    "\n",
    "This notebook is based on the original example: https://github.com/yandex-research/tabm/blob/main/example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yandex-research/tabm\n",
    "!pip install wldhx.yadisk-direct rtdl_num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:30.804003Z",
     "iopub.status.busy": "2024-12-16T06:38:30.803394Z",
     "iopub.status.idle": "2024-12-16T06:38:33.044566Z",
     "shell.execute_reply": "2024-12-16T06:38:33.043646Z",
     "shell.execute_reply.started": "2024-12-16T06:38:30.803929Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OrdinalEncoder\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "from tabm_reference import Model, make_parameter_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:36.063330Z",
     "iopub.status.busy": "2024-12-16T06:38:36.062985Z",
     "iopub.status.idle": "2024-12-16T06:38:36.068873Z",
     "shell.execute_reply": "2024-12-16T06:38:36.067895Z",
     "shell.execute_reply.started": "2024-12-16T06:38:36.063304Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:37.053142Z",
     "iopub.status.busy": "2024-12-16T06:38:37.052051Z",
     "iopub.status.idle": "2024-12-16T06:38:38.847409Z",
     "shell.execute_reply": "2024-12-16T06:38:38.846505Z",
     "shell.execute_reply.started": "2024-12-16T06:38:37.053051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:        CUDA\n",
      "AMP:           False (dtype: torch.bfloat16)\n",
      "torch.compile: False\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Automatic mixed precision (AMP)\n",
    "# torch.float16 is implemented for completeness,\n",
    "# but it was not tested in the project,\n",
    "# so torch.bfloat16 is used by default.\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float16\n",
    "    if torch.cuda.is_available()\n",
    "    else None\n",
    ")\n",
    "# Changing False to True will result in faster training on compatible hardware.\n",
    "amp_enabled = False and amp_dtype is not None\n",
    "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
    "\n",
    "# torch.compile\n",
    "compile_model = False\n",
    "\n",
    "# fmt: off\n",
    "print(\n",
    "    f'Device:        {device.type.upper()}'\n",
    "    f'\\nAMP:           {amp_enabled} (dtype: {amp_dtype})'\n",
    "    f'\\ntorch.compile: {compile_model}'\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Dataset is \"Regression with an Insurance Dataset\"\n",
    "\n",
    "https://www.kaggle.com/competitions/playground-series-s4e12/overview\n",
    "\n",
    "if you have a kaggle account, you can download the data using kaggle API:\n",
    "\n",
    "```python\n",
    "kaggle competitions download -c playground-series-s4e12\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T16:14:42.574928Z",
     "iopub.status.busy": "2024-12-15T16:14:42.574608Z",
     "iopub.status.idle": "2024-12-15T16:14:48.784632Z",
     "shell.execute_reply": "2024-12-15T16:14:48.783365Z",
     "shell.execute_reply.started": "2024-12-15T16:14:42.574898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  185M  100  185M    0     0  65.3M      0  0:00:02  0:00:02 --:--:-- 82.5M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  119M  100  119M    0     0  64.5M      0  0:00:01  0:00:01 --:--:-- 70.3M\n"
     ]
    }
   ],
   "source": [
    "! curl -L $(yadisk-direct https://disk.yandex.ru/d/YbkU_KfAqGtdXg) -o insurance_dataset_train.csv\n",
    "! curl -L $(yadisk-direct https://disk.yandex.ru/d/_gs3p3yvp0TNRg) -o insurance_dataset_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:41.938375Z",
     "iopub.status.busy": "2024-12-16T06:38:41.937020Z",
     "iopub.status.idle": "2024-12-16T06:38:46.524284Z",
     "shell.execute_reply": "2024-12-16T06:38:46.522910Z",
     "shell.execute_reply.started": "2024-12-16T06:38:41.938309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upload train and test parts\n",
    "train_df = pd.read_csv(\"./insurance_dataset_train.csv\")\n",
    "test_df = pd.read_csv(\"./insurance_dataset_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define lists with numerical and categorical column names.\n",
    "\n",
    "Also we exclude `id` and `Policy Start Date` cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:46.525916Z",
     "iopub.status.busy": "2024-12-16T06:38:46.525699Z",
     "iopub.status.idle": "2024-12-16T06:38:46.530661Z",
     "shell.execute_reply": "2024-12-16T06:38:46.529844Z",
     "shell.execute_reply.started": "2024-12-16T06:38:46.525894Z"
    }
   },
   "outputs": [],
   "source": [
    "target_col = \"Premium Amount\"\n",
    "\n",
    "num_cols = ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', \n",
    "            'Previous Claims', 'Vehicle Age', 'Credit Score', 'Insurance Duration']\n",
    "cat_cols = ['Gender', 'Marital Status', 'Education Level', 'Occupation', 'Location',\n",
    "            'Policy Type', 'Customer Feedback', 'Smoking Status', 'Exercise Frequency',\n",
    "            'Property Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train part on train and val, test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:46.531435Z",
     "iopub.status.busy": "2024-12-16T06:38:46.531255Z",
     "iopub.status.idle": "2024-12-16T06:38:46.535079Z",
     "shell.execute_reply": "2024-12-16T06:38:46.534298Z",
     "shell.execute_reply.started": "2024-12-16T06:38:46.531414Z"
    }
   },
   "outputs": [],
   "source": [
    "# If training is TOO slow, set to True\n",
    "DEMO_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:46.536438Z",
     "iopub.status.busy": "2024-12-16T06:38:46.536140Z",
     "iopub.status.idle": "2024-12-16T06:38:47.301588Z",
     "shell.execute_reply": "2024-12-16T06:38:47.300629Z",
     "shell.execute_reply.started": "2024-12-16T06:38:46.536418Z"
    }
   },
   "outputs": [],
   "source": [
    "# collect number of unique values for categorical columns. Will be needed in future.\n",
    "cat_cardinalities = train_df[cat_cols].nunique(dropna=False).to_list()\n",
    "\n",
    "if DEMO_MODE:\n",
    "    idx = np.random.randint(0, len(train_df), size=50000)\n",
    "    train_df = train_df.iloc[idx]\n",
    "\n",
    "all_idx = np.arange(len(train_df))\n",
    "# Select the test part\n",
    "train_idx, val_idx = train_test_split(all_idx, train_size=0.8)\n",
    "\n",
    "# Fill dict with all parts we have\n",
    "_train_df = train_df.iloc[train_idx]\n",
    "_val_df = train_df.iloc[val_idx]\n",
    "\n",
    "data = {\n",
    "    'train': {\n",
    "        'x_num': _train_df[num_cols],\n",
    "        'x_cat': _train_df[cat_cols],\n",
    "        'y': _train_df[target_col].to_numpy().astype(np.float32)\n",
    "    },\n",
    "    'val': {\n",
    "        'x_num': _val_df[num_cols],\n",
    "        'x_cat': _val_df[cat_cols],\n",
    "        'y': _val_df[target_col].to_numpy().astype(np.float32)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Also process test data, we'll need it in the end\n",
    "test_data = {\n",
    "    \"x_num\": test_df[num_cols],\n",
    "    \"x_cat\": test_df[cat_cols],\n",
    "    'id': test_df['id'].to_numpy()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define processing pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:47.303379Z",
     "iopub.status.busy": "2024-12-16T06:38:47.302735Z",
     "iopub.status.idle": "2024-12-16T06:38:47.308211Z",
     "shell.execute_reply": "2024-12-16T06:38:47.307244Z",
     "shell.execute_reply.started": "2024-12-16T06:38:47.303356Z"
    }
   },
   "outputs": [],
   "source": [
    "n_quantiles = max(min(len(train_idx) // 30, 1000), 10)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", QuantileTransformer(n_quantiles=n_quantiles, \n",
    "                                   output_distribution='normal',\n",
    "                                   subsample=10**9))\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply processing operations to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:47.745937Z",
     "iopub.status.busy": "2024-12-16T06:38:47.745710Z",
     "iopub.status.idle": "2024-12-16T06:38:50.490625Z",
     "shell.execute_reply": "2024-12-16T06:38:50.489618Z",
     "shell.execute_reply.started": "2024-12-16T06:38:47.745914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`x_num` dtype: <class 'numpy.ndarray'>\n",
      "`x_cat` dtype: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# The noise is added to improve the output of QuantileTransformer in some cases\n",
    "noise = (\n",
    "    np.random.default_rng(0)\n",
    "    .normal(0.0, 1e-5, data['train']['x_num'].shape)\n",
    ")\n",
    "\n",
    "# Fit pipelines on train part\n",
    "num_processor = num_pipeline.fit(data['train']['x_num'] + noise)\n",
    "cat_processor = cat_pipeline.fit(data['train']['x_cat'])\n",
    "\n",
    "# Apply the processing to all parts. Note, that when processor is applied\n",
    "# each dataframe converts to np.ndarray. Check that x_num array has float32 dtype\n",
    "# and x_cat is int64.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "print(f\"`x_num` dtype: {type(data['train']['x_num'])}\\n`x_cat` dtype: {type(data['train']['x_cat'])}\")\n",
    "\n",
    "# Apply processing to test data as well\n",
    "\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert every part of the dataset to `torch.tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:50.492105Z",
     "iopub.status.busy": "2024-12-16T06:38:50.491896Z",
     "iopub.status.idle": "2024-12-16T06:38:53.373468Z",
     "shell.execute_reply": "2024-12-16T06:38:53.372346Z",
     "shell.execute_reply.started": "2024-12-16T06:38:50.492083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "data_torch = {\n",
    "    part: {k: torch.as_tensor(v, device=device) for k, v in data[part].items()}\n",
    "    for part in data\n",
    "}\n",
    "\n",
    "test_data_torch = {\n",
    "    \"x_num\": torch.as_tensor(test_data[\"x_num\"], device=device),\n",
    "    \"x_cat\": torch.as_tensor(test_data[\"x_cat\"], device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:53.374457Z",
     "iopub.status.busy": "2024-12-16T06:38:53.374252Z",
     "iopub.status.idle": "2024-12-16T06:38:53.411223Z",
     "shell.execute_reply": "2024-12-16T06:38:53.410380Z",
     "shell.execute_reply.started": "2024-12-16T06:38:53.374436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose one of the two configurations below.\n",
    "\n",
    "# TabM or TabM-mini\n",
    "# arch_type = 'tabm'\n",
    "arch_type = 'tabm-mini'\n",
    "\n",
    "# Use the piecewise-linear embeddings\n",
    "bins = rtdl_num_embeddings.compute_bins(data_torch['train']['x_num'])\n",
    "# bins = None\n",
    "\n",
    "model = Model(\n",
    "    n_num_features=len(num_cols),\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    n_classes=None,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 16,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=32,\n",
    ").to(device)\n",
    "\n",
    "if compile_model:\n",
    "    # NOTE\n",
    "    # `torch.compile` is intentionally called without the `mode` argument\n",
    "    # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n",
    "    model = torch.compile(model)\n",
    "    evaluation_mode = torch.no_grad\n",
    "else:\n",
    "    evaluation_mode = torch.inference_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:53.412590Z",
     "iopub.status.busy": "2024-12-16T06:38:53.412406Z",
     "iopub.status.idle": "2024-12-16T06:38:53.423679Z",
     "shell.execute_reply": "2024-12-16T06:38:53.422890Z",
     "shell.execute_reply.started": "2024-12-16T06:38:53.412569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params:  373568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (num_module): PiecewiseLinearEmbeddings(\n",
       "    (linear0): LinearEmbeddings()\n",
       "    (impl): _PiecewiseLinearEncodingImpl()\n",
       "    (linear): _NLinear()\n",
       "  )\n",
       "  (cat_module): OneHotEncoding0d()\n",
       "  (backbone): MLP(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=161, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (minimal_ensemble_adapter): ScaleEnsemble()\n",
       "  (output): NLinear()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of params: \", sum(p.numel() for p in model.parameters()))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Loss in kaggle competition is RMSLE (Root Mean Squared Logarithmic Error) let's use it as validation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:53.424583Z",
     "iopub.status.busy": "2024-12-16T06:38:53.424406Z",
     "iopub.status.idle": "2024-12-16T06:38:53.428447Z",
     "shell.execute_reply": "2024-12-16T06:38:53.427663Z",
     "shell.execute_reply.started": "2024-12-16T06:38:53.424563Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "def compute_rmsle(y_pred, y_true):\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    return root_mean_squared_log_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define class for training and evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:47:10.091841Z",
     "iopub.status.busy": "2024-12-16T06:47:10.091546Z",
     "iopub.status.idle": "2024-12-16T06:47:10.125455Z",
     "shell.execute_reply": "2024-12-16T06:47:10.123926Z",
     "shell.execute_reply.started": "2024-12-16T06:47:10.091813Z"
    }
   },
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    \"\"\"Runner for train/evaludate and predict using provided model.\"\"\"\n",
    "    def __init__(self, model, optimizer, loss, device, eval_metric=None, grad_scaler=None, \n",
    "                 epoch_bar=False, checkpoint_name=\"tabm_model.ckpt\"):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.device = device\n",
    "        self.eval_metric = loss if eval_metric is None else eval_metric\n",
    "        self.grad_scaler = grad_scaler\n",
    "        self.epoch_bar = epoch_bar\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "\n",
    "        self._train_mean = None\n",
    "        self._train_std = None\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        # TabM produces k predictions per object. Each of them must be trained separately.\n",
    "        # (regression)     y_pred.shape == (batch_size, k)\n",
    "        # (classification) y_pred.shape == (batch_size, k, n_classes)\n",
    "        k = y_pred.shape[-1]\n",
    "        return self.loss(y_pred.flatten(0, 1), y_true.repeat_interleave(k))\n",
    "\n",
    "    def forward(self, batch_num, batch_cat=None, model=None):\n",
    "        model = self.model if model is None else model\n",
    "        return (\n",
    "            model(batch_num, batch_cat)\n",
    "            .squeeze(-1)\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "    def _train_step(self, y_pred, y_true, normalize_target):\n",
    "        # We need to normalize target if needed and compute loss\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        if self.grad_scaler is None:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        else:\n",
    "            self.grad_scaler.scale(loss).backward()\n",
    "            self.grad_scaler.step(optimizer)\n",
    "            self.grad_scaler.update()\n",
    "        self.optimizer.zero_grad()\n",
    "        return loss.detach().cpu().numpy().mean()\n",
    "    \n",
    "    def _eval_step(self, y_pred, y_true):\n",
    "        # Compute the mean of the k predictions and scale prediction \n",
    "        # into it's original range\n",
    "\n",
    "        y_pred = ### YOUR CODE HERE\n",
    "\n",
    "        # Avoid computing gradients if train_phase is False\n",
    "        with torch.set_grad_enabled(False):\n",
    "            metric = self.eval_metric(y_pred, y_true)\n",
    "        if isinstance(metric, torch.Tensor):\n",
    "            return metric.detach().cpu().numpy()\n",
    "        return metric\n",
    "\n",
    "    def _run_epoch(self, data, batch_indices, train_phase=True, normalize_target=True, model=None):\n",
    "        self.model.train(train_phase)\n",
    "        epoch_history = []\n",
    "        for batch_idx in tqdm(batch_indices, disable=not self.epoch_bar, leave=False):\n",
    "            batch_num = data['x_num'][batch_idx]\n",
    "            batch_cat = data['x_cat'][batch_idx]\n",
    "            y_true = data['y'][batch_idx]\n",
    "            y_pred = self.forward(batch_num, batch_cat, model=model)\n",
    "\n",
    "            if train_phase:\n",
    "                res = self._train_step(y_pred, y_true, normalize_target=normalize_target)\n",
    "            else:\n",
    "                res = self._eval_step(y_pred, y_true)\n",
    "            epoch_history.append(res)\n",
    "        return np.mean(epoch_history)\n",
    "\n",
    "    def train(self, train_data, val_data, batch_size=256, n_epochs=1000, patience=16,\n",
    "              eval_batch_size=None, normalize_target=True, save_best_model=False, \n",
    "              force_collect_stats=False):\n",
    "        best = {\n",
    "            'val': math.inf,\n",
    "            'test': math.inf,\n",
    "            'epoch': -1,\n",
    "        }\n",
    "\n",
    "        # Important!\n",
    "        # For regression tasks it is highly recommended to standardize the training labels.\n",
    "        self._train_mean = (train_data['y'].mean() \n",
    "                            if self._train_mean is None or force_collect_stats \n",
    "                            else self._train_mean)\n",
    "        self._train_std = (train_data['y'].std() \n",
    "                           if self._train_std is None or force_collect_stats \n",
    "                           else self._train_std)\n",
    "\n",
    "        # Early stopping: the training stops when\n",
    "        # there are more than `patience` consequtive bad updates.\n",
    "        remaining_patience = patience\n",
    "        eval_batch_size = batch_size if eval_batch_size is None else eval_batch_size\n",
    "        for epoch in tqdm(range(n_epochs), total=n_epochs):\n",
    "            # Generate batch indices\n",
    "            batch_indices = torch.randperm(len(train_data['y']), device=device).split(batch_size)\n",
    "            # Perform train epoch\n",
    "            train_score = self._run_epoch(train_data, batch_indices, train_phase=True,\n",
    "                                          normalize_target=normalize_target)\n",
    "            # Validate model after train epoch\n",
    "            val_score = self.evaluate(val_data, eval_batch_size)\n",
    "\n",
    "            msg = f\"Epoch: {epoch} (train) {train_score:.4f} (val) {val_score:.4f}\"\n",
    "        \n",
    "            if val_score < best['val']:\n",
    "                best = {'val': val_score, 'epoch': epoch}\n",
    "                remaining_patience = patience\n",
    "                print(msg + \"\\tðŸŒ¸ New best epoch! ðŸŒ¸\")\n",
    "                if save_best_model:\n",
    "                    torch.save(self.model, open(self.checkpoint_name, 'wb'))\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "                print(msg)\n",
    "        \n",
    "            if remaining_patience < 0:\n",
    "                break\n",
    "        print(f\"\\nTraining is complete.\\nBest model: epoch {best['epoch']} metric {best['val']}\")\n",
    "\n",
    "    @evaluation_mode\n",
    "    def evaluate(self, data, batch_size=256, model=None):\n",
    "        batch_indices = torch.randperm(len(data['y']), device=device).split(batch_size)\n",
    "        return self._run_epoch(data, batch_indices, train_phase=False, model=model)\n",
    "\n",
    "    @evaluation_mode\n",
    "    def predict(self, data, batch_size=256, model=None):\n",
    "        batch_indices = torch.arange(len(data['x_num']), device=device).split(batch_size)\n",
    "        preds = []\n",
    "        for batch_idx in tqdm(batch_indices, disable=not self.epoch_bar, leave=False):\n",
    "            batch_num = data['x_num'][batch_idx]\n",
    "            batch_cat = data['x_cat'][batch_idx]\n",
    "            # You need to make prediction for batch and store result in `preds`\n",
    "            # Don't forget to scale prediction into it's orginal range\n",
    "            y_pred = ### YOUR CODE HERE\n",
    "\n",
    "            preds.extend(y_pred.detach().cpu().numpy())\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create runner instance and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:53.444383Z",
     "iopub.status.busy": "2024-12-16T06:38:53.444210Z",
     "iopub.status.idle": "2024-12-16T06:38:54.102010Z",
     "shell.execute_reply": "2024-12-16T06:38:54.101106Z",
     "shell.execute_reply.started": "2024-12-16T06:38:53.444364Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=2e-3, weight_decay=3e-4)\n",
    "loss = F.mse_loss\n",
    "eval_metric = compute_rmsle\n",
    "\n",
    "runner = Runner(model, optimizer, loss, device=device, eval_metric=eval_metric,\n",
    "                grad_scaler=grad_scaler, epoch_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:38:54.134276Z",
     "iopub.status.busy": "2024-12-16T06:38:54.133995Z",
     "iopub.status.idle": "2024-12-16T06:39:02.078424Z",
     "shell.execute_reply": "2024-12-16T06:39:02.077478Z",
     "shell.execute_reply.started": "2024-12-16T06:38:54.134253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b1e8e8e54441f1a303fce4dfabf4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 (train) 1.0004 (val) 1.1702\tðŸŒ¸ New best epoch! ðŸŒ¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (train) 0.9984 (val) 1.1611\tðŸŒ¸ New best epoch! ðŸŒ¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (train) 0.9958 (val) 1.1669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 (train) 0.9942 (val) 1.1566\tðŸŒ¸ New best epoch! ðŸŒ¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 (train) 0.9929 (val) 1.1663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 (train) 0.9900 (val) 1.1541\tðŸŒ¸ New best epoch! ðŸŒ¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 (train) 0.9865 (val) 1.1585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 (train) 0.9846 (val) 1.1549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 (train) 0.9811 (val) 1.1528\tðŸŒ¸ New best epoch! ðŸŒ¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 (train) 0.9781 (val) 1.1552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 (train) 0.9758 (val) 1.1511\tðŸŒ¸ New best epoch! ðŸŒ¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 (train) 0.9749 (val) 1.1411\tðŸŒ¸ New best epoch! ðŸŒ¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 (train) 0.9693 (val) 1.1367\tðŸŒ¸ New best epoch! ðŸŒ¸\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 (train) 0.9670 (val) 1.1553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 (train) 0.9666 (val) 1.1489\n",
      "Training is complete.\n",
      " Best model: epoch 12 metric 1.136688470840454\n"
     ]
    }
   ],
   "source": [
    "runner.train(data_torch['train'], data_torch['val'], batch_size=1024, n_epochs=15, save_best_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to improve the result??\n",
    "* Play with network params\n",
    "* Try to use or dont use embeddings\n",
    "* Try different models (tabM, tabM_mini)\n",
    "* (*) Tune model params using optuna ([source](https://optuna.org/), [examples](https://github.com/optuna/optuna-examples))\n",
    "* Any other ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about standard approaches? Let's try catboost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using colab\n",
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:39:27.334135Z",
     "iopub.status.busy": "2024-12-16T06:39:27.333796Z",
     "iopub.status.idle": "2024-12-16T06:39:48.807134Z",
     "shell.execute_reply": "2024-12-16T06:39:48.806439Z",
     "shell.execute_reply.started": "2024-12-16T06:39:27.334103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.067434\n",
      "0:\tlearn: 864.9865538\ttotal: 11.8ms\tremaining: 11.8s\n",
      "50:\tlearn: 849.9454936\ttotal: 1.27s\tremaining: 23.7s\n",
      "100:\tlearn: 845.9937747\ttotal: 1.81s\tremaining: 16.1s\n",
      "150:\tlearn: 841.7127103\ttotal: 2.98s\tremaining: 16.7s\n",
      "200:\tlearn: 838.3827540\ttotal: 4.14s\tremaining: 16.5s\n",
      "250:\tlearn: 834.8489806\ttotal: 4.79s\tremaining: 14.3s\n",
      "300:\tlearn: 831.5816795\ttotal: 5.59s\tremaining: 13s\n",
      "350:\tlearn: 828.3659713\ttotal: 6.81s\tremaining: 12.6s\n",
      "400:\tlearn: 825.5104448\ttotal: 7.35s\tremaining: 11s\n",
      "450:\tlearn: 822.5501248\ttotal: 8.58s\tremaining: 10.4s\n",
      "500:\tlearn: 819.7249818\ttotal: 9.84s\tremaining: 9.8s\n",
      "550:\tlearn: 817.0348387\ttotal: 11.1s\tremaining: 9.01s\n",
      "600:\tlearn: 814.4520659\ttotal: 11.6s\tremaining: 7.7s\n",
      "650:\tlearn: 811.9044641\ttotal: 12.8s\tremaining: 6.87s\n",
      "700:\tlearn: 809.4678153\ttotal: 14s\tremaining: 5.97s\n",
      "750:\tlearn: 806.8525968\ttotal: 14.8s\tremaining: 4.92s\n",
      "800:\tlearn: 804.4646745\ttotal: 15.7s\tremaining: 3.91s\n",
      "850:\tlearn: 802.2513984\ttotal: 17s\tremaining: 2.97s\n",
      "900:\tlearn: 799.6769396\ttotal: 18.2s\tremaining: 2s\n",
      "950:\tlearn: 797.3774871\ttotal: 18.7s\tremaining: 964ms\n",
      "999:\tlearn: 794.9690127\ttotal: 19.9s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7fec8e456b40>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "kwargs = {\"task_type\": 'GPU', \"devices\": \"0\"} if torch.cuda.is_available() else {}\n",
    "catboost_model = CatBoostRegressor(**kwargs, iterations=500)\n",
    "\n",
    "X_cb = train_df[cat_cols + num_cols].fillna(0)\n",
    "y_cb = train_df[target_col]\n",
    "catboost_model.fit(X_cb, y_cb, cat_features=cat_cols, verbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:39:48.808665Z",
     "iopub.status.busy": "2024-12-16T06:39:48.808379Z",
     "iopub.status.idle": "2024-12-16T06:39:50.434159Z",
     "shell.execute_reply": "2024-12-16T06:39:50.433270Z",
     "shell.execute_reply.started": "2024-12-16T06:39:48.808641Z"
    }
   },
   "outputs": [],
   "source": [
    "catboost_preds = catboost_model.predict(test_df[cat_cols + num_cols].fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission\n",
    "\n",
    "Tips:\n",
    "* Save your local \"best\" results, to prevent overfitting on validation part\n",
    "* (simple) Before making submission (after params tuning), train model using whole train part (insurance_dataset_train)\n",
    "* (harder) Make submission as an ensemble of cross-validated models on the train part of the dataset\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:39:50.435167Z",
     "iopub.status.busy": "2024-12-16T06:39:50.434964Z",
     "iopub.status.idle": "2024-12-16T06:39:50.439225Z",
     "shell.execute_reply": "2024-12-16T06:39:50.438448Z",
     "shell.execute_reply.started": "2024-12-16T06:39:50.435146Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_submission(preds, ids, path=None):\n",
    "    result = pd.DataFrame({\"id\": ids, \"Premium Amount\": preds})\n",
    "    if path is None:\n",
    "        return result\n",
    "    result.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model from checkpoint\n",
    "# with open(\"tabm_model.ckpt\", 'rb') as f:\n",
    "#     best_model = torch.load(f)\n",
    "# preds = runner.predict(test_data_torch, batch_size=2048, model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use model from last epoch\n",
    "preds = runner.predict(test_data_torch, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(preds, test_data[\"id\"], \"simple_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(catboost_preds, test_df[\"id\"], \"simple_catboost_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit using API\n",
    "\n",
    "You can submit your solution directly from colab!\n",
    "\n",
    "\n",
    "```python\n",
    "kaggle competitions submit -c playground-series-s4e12 -f submission.csv -m \"Message\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
