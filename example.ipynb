{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/yandex-research/tabm/blob/main/example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# TabM\n",
    "\n",
    "This notebook provides a usage example of the `tabm` package from the\n",
    "[TabM](https://github.com/yandex-research/tabm) project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rtdl_num_embeddings\n",
    "!pip install tabm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Any, Literal, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tabm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train    x_num    (13209, 8)    float32\n",
      "train    y        (13209,)      float32\n",
      "val      x_num    (3303, 8)     float32\n",
      "val      y        (3303,)       float32\n",
      "test     x_num    (4128, 8)     float32\n",
      "test     y        (4128,)       float32\n"
     ]
    }
   ],
   "source": [
    "# >>> Dataset.\n",
    "TaskType = Literal['regression', 'binclass', 'multiclass']\n",
    "\n",
    "# Regression.\n",
    "task_type: TaskType = 'regression'\n",
    "n_classes = None\n",
    "dataset = sklearn.datasets.fetch_california_housing()\n",
    "X_num: np.ndarray = dataset['data']\n",
    "Y: np.ndarray = dataset['target']\n",
    "\n",
    "# Classification.\n",
    "# n_classes = 2\n",
    "# assert n_classes >= 2\n",
    "# task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
    "# x_num, Y = sklearn.datasets.make_classification(\n",
    "#     n_samples=20000,\n",
    "#     n_features=8,\n",
    "#     n_classes=n_classes,\n",
    "#     n_informative=3,\n",
    "#     n_redundant=2,\n",
    "# )\n",
    "\n",
    "task_is_regression = task_type == 'regression'\n",
    "\n",
    "# >>> Numerical (continuous) features.\n",
    "X_num: np.ndarray = X_num.astype(np.float32)\n",
    "n_num_features = X_num.shape[1]\n",
    "\n",
    "# >>> Categorical features.\n",
    "# NOTE: the above datasets do not have categorical features, however,\n",
    "# for the demonstration purposes, it is possible to generate them.\n",
    "cat_cardinalities = [\n",
    "    # NOTE: uncomment the two lines below to add two categorical features.\n",
    "    # 4,  # Allowed values: [0, 1, 2, 3].\n",
    "    # 7,  # Allowed values: [0, 1, 2, 3, 4, 5, 6].\n",
    "]\n",
    "X_cat = (\n",
    "    np.column_stack([np.random.randint(0, c, (len(X_num),)) for c in cat_cardinalities])\n",
    "    if cat_cardinalities\n",
    "    else None\n",
    ")\n",
    "\n",
    "# >>> Labels.\n",
    "if task_type == 'regression':\n",
    "    Y = Y.astype(np.float32)\n",
    "else:\n",
    "    assert n_classes is not None\n",
    "    Y = Y.astype(np.int64)\n",
    "    assert set(Y.tolist()) == set(range(n_classes)), (\n",
    "        'Classification labels must form the range [0, 1, ..., n_classes - 1]'\n",
    "    )\n",
    "\n",
    "# >>> Split the dataset.\n",
    "all_idx = np.arange(len(Y))\n",
    "trainval_idx, test_idx = sklearn.model_selection.train_test_split(\n",
    "    all_idx, train_size=0.8\n",
    ")\n",
    "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
    "    trainval_idx, train_size=0.8\n",
    ")\n",
    "data_numpy = {\n",
    "    'train': {'x_num': X_num[train_idx], 'y': Y[train_idx]},\n",
    "    'val': {'x_num': X_num[val_idx], 'y': Y[val_idx]},\n",
    "    'test': {'x_num': X_num[test_idx], 'y': Y[test_idx]},\n",
    "}\n",
    "if X_cat is not None:\n",
    "    data_numpy['train']['x_cat'] = X_cat[train_idx]\n",
    "    data_numpy['val']['x_cat'] = X_cat[val_idx]\n",
    "    data_numpy['test']['x_cat'] = X_cat[test_idx]\n",
    "\n",
    "for part, part_data in data_numpy.items():\n",
    "    for key, value in part_data.items():\n",
    "        print(f'{part:<5}    {key:<5}    {value.shape!r:<10}    {value.dtype}')\n",
    "        del key, value\n",
    "    del part, part_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preprocessing.\n",
    "# NOTE\n",
    "# The choice between preprocessing strategies depends on a task and a model.\n",
    "\n",
    "# Simple preprocessing strategy.\n",
    "# preprocessing = sklearn.preprocessing.StandardScaler().fit(\n",
    "#     data_numpy['train']['x_num']\n",
    "# )\n",
    "\n",
    "# Advanced preprocessing strategy.\n",
    "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
    "x_num_train_numpy = data_numpy['train']['x_num']\n",
    "noise = (\n",
    "    np.random.default_rng(0)\n",
    "    .normal(0.0, 1e-5, x_num_train_numpy.shape)\n",
    "    .astype(x_num_train_numpy.dtype)\n",
    ")\n",
    "preprocessing = sklearn.preprocessing.QuantileTransformer(\n",
    "    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),\n",
    "    output_distribution='normal',\n",
    "    subsample=10**9,\n",
    ").fit(x_num_train_numpy + noise)\n",
    "del x_num_train_numpy\n",
    "\n",
    "# Apply the preprocessing.\n",
    "for part in data_numpy:\n",
    "    data_numpy[part]['x_num'] = preprocessing.transform(data_numpy[part]['x_num'])\n",
    "\n",
    "\n",
    "# Label preprocessing.\n",
    "class RegressionLabelStats(NamedTuple):\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "\n",
    "Y_train = data_numpy['train']['y'].copy()\n",
    "if task_type == 'regression':\n",
    "    # For regression tasks, it is highly recommended to standardize the training labels.\n",
    "    regression_label_stats = RegressionLabelStats(\n",
    "        Y_train.mean().item(), Y_train.std().item()\n",
    "    )\n",
    "    Y_train = (Y_train - regression_label_stats.mean) / regression_label_stats.std\n",
    "else:\n",
    "    regression_label_stats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PyTorch settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:        CUDA\n",
      "AMP:           False\n",
      "torch.compile: False\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert data to tensors\n",
    "data = {\n",
    "    part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
    "    for part in data_numpy\n",
    "}\n",
    "Y_train = torch.as_tensor(Y_train, device=device)\n",
    "if task_type == 'regression':\n",
    "    for part in data:\n",
    "        data[part]['y'] = data[part]['y'].float()\n",
    "    Y_train = Y_train.float()\n",
    "\n",
    "# Automatic mixed precision (AMP)\n",
    "# torch.float16 is implemented for completeness,\n",
    "# but it was not tested in the project,\n",
    "# so torch.bfloat16 is used by default.\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float16\n",
    "    if torch.cuda.is_available()\n",
    "    else None\n",
    ")\n",
    "# Changing False to True can speed up training\n",
    "# of large enough models on compatible hardware.\n",
    "amp_enabled = False and amp_dtype is not None\n",
    "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
    "\n",
    "# torch.compile\n",
    "compile_model = False\n",
    "\n",
    "# fmt: off\n",
    "print(f'Device:        {device.type.upper()}')\n",
    "print(f'AMP:           {amp_enabled}{f\" ({amp_dtype})\"if amp_enabled else \"\"}')\n",
    "print(f'torch.compile: {compile_model}')\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The best performance is usually achieved with `num_embeddings`\n",
    "from the `rtdl_num_embeddings` package. Typically, `PiecewiseLinearEmbeddings`\n",
    "and `PeriodicEmbeddings` perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No embeddings.\n",
    "num_embeddings = None\n",
    "\n",
    "# Simple embeddings.\n",
    "num_embeddings = rtdl_num_embeddings.LinearReLUEmbeddings(n_num_features)\n",
    "\n",
    "# Periodic embeddings.\n",
    "num_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(n_num_features, lite=False)\n",
    "\n",
    "# Piecewise-linear embeddings.\n",
    "num_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "    rtdl_num_embeddings.compute_bins(data['train']['x_num'], n_bins=48),\n",
    "    d_embedding=16,\n",
    "    activation=False,\n",
    "    version='B',\n",
    ")\n",
    "\n",
    "model = tabm.TabM.make(\n",
    "    n_num_features=n_num_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=1 if n_classes is None else n_classes,\n",
    "    num_embeddings=num_embeddings,\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=3e-4)\n",
    "\n",
    "if compile_model:\n",
    "    # NOTE\n",
    "    # `torch.compile(model, mode=\"reduce-overhead\")` caused issues during training,\n",
    "    # so the `mode` argument is not used.\n",
    "    model = torch.compile(model)\n",
    "    evaluation_mode = torch.no_grad\n",
    "else:\n",
    "    evaluation_mode = torch.inference_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick reminder: TabM represents an ensemble of k MLPs.\n",
    "#\n",
    "# The option below determines if the MLPs are trained\n",
    "# on the same batches (share_training_batches=True) or\n",
    "# on different batches. Technically, this option determines:\n",
    "# - How the loss function is implemented.\n",
    "# - How the training batches are constructed.\n",
    "#\n",
    "# `True` is recommended by default because of better training efficiency.\n",
    "# On some tasks, `False` may provide better performance.\n",
    "share_training_batches = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score before training: -1.1452\n"
     ]
    }
   ],
   "source": [
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[part]['x_num'][idx],\n",
    "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
    "        )\n",
    "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "\n",
    "base_loss_fn = F.mse_loss if task_is_regression else F.cross_entropy\n",
    "\n",
    "\n",
    "def loss_fn(y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
    "    # TabM produces k predictions. Each of them must be trained separately.\n",
    "\n",
    "    # Regression:     (batch_size, k)            -> (batch_size * k,)\n",
    "    # Classification: (batch_size, k, n_classes) -> (batch_size * k, n_classes)\n",
    "    y_pred = y_pred.flatten(0, 1)\n",
    "\n",
    "    if share_training_batches:\n",
    "        # (batch_size,) -> (batch_size * k,)\n",
    "        y_true = y_true.repeat_interleave(model.backbone.k)\n",
    "    else:\n",
    "        # (batch_size, k) -> (batch_size * k,)\n",
    "        y_true = y_true.flatten(0, 1)\n",
    "\n",
    "    return base_loss_fn(y_pred, y_true)\n",
    "\n",
    "\n",
    "@evaluation_mode()\n",
    "def evaluate(part: str) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    # When using torch.compile, you may need to reduce the evaluation batch size.\n",
    "    eval_batch_size = 8096\n",
    "    y_pred: np.ndarray = (\n",
    "        torch.cat(\n",
    "            [\n",
    "                apply_model(part, idx)\n",
    "                for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
    "                    eval_batch_size\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    if task_type == 'regression':\n",
    "        # Transform the predictions back to the original label space.\n",
    "        assert regression_label_stats is not None\n",
    "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
    "\n",
    "    # Compute the mean of the k predictions.\n",
    "    if not task_is_regression:\n",
    "        # For classification, the mean must be computed in the probability space.\n",
    "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
    "    y_pred = y_pred.mean(1)\n",
    "\n",
    "    y_true = data[part]['y'].cpu().numpy()\n",
    "    score = (\n",
    "        -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "        if task_type == 'regression'\n",
    "        else sklearn.metrics.accuracy_score(y_true, y_pred.argmax(1))\n",
    "    )\n",
    "    return float(score)  # The higher -- the better.\n",
    "\n",
    "\n",
    "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [epoch] 0   [val] -0.599 [test] -0.608\n",
      "* [epoch] 1   [val] -0.568 [test] -0.578\n",
      "* [epoch] 2   [val] -0.559 [test] -0.568\n",
      "* [epoch] 3   [val] -0.534 [test] -0.542\n",
      "  [epoch] 4   [val] -0.552 [test] -0.555\n",
      "* [epoch] 5   [val] -0.527 [test] -0.532\n",
      "* [epoch] 6   [val] -0.515 [test] -0.521\n",
      "* [epoch] 7   [val] -0.514 [test] -0.516\n",
      "* [epoch] 8   [val] -0.506 [test] -0.513\n",
      "* [epoch] 9   [val] -0.495 [test] -0.499\n",
      "  [epoch] 10  [val] -0.507 [test] -0.511\n",
      "* [epoch] 11  [val] -0.490 [test] -0.494\n",
      "  [epoch] 12  [val] -0.502 [test] -0.506\n",
      "* [epoch] 13  [val] -0.488 [test] -0.487\n",
      "* [epoch] 14  [val] -0.486 [test] -0.485\n",
      "  [epoch] 15  [val] -0.490 [test] -0.491\n",
      "  [epoch] 16  [val] -0.492 [test] -0.492\n",
      "* [epoch] 17  [val] -0.478 [test] -0.480\n",
      "  [epoch] 18  [val] -0.479 [test] -0.479\n",
      "* [epoch] 19  [val] -0.474 [test] -0.472\n",
      "* [epoch] 20  [val] -0.465 [test] -0.469\n",
      "  [epoch] 21  [val] -0.477 [test] -0.476\n",
      "* [epoch] 22  [val] -0.464 [test] -0.462\n",
      "  [epoch] 23  [val] -0.474 [test] -0.473\n",
      "* [epoch] 24  [val] -0.462 [test] -0.463\n",
      "* [epoch] 25  [val] -0.461 [test] -0.462\n",
      "  [epoch] 26  [val] -0.464 [test] -0.463\n",
      "  [epoch] 27  [val] -0.472 [test] -0.468\n",
      "* [epoch] 28  [val] -0.457 [test] -0.459\n",
      "* [epoch] 29  [val] -0.456 [test] -0.458\n",
      "  [epoch] 30  [val] -0.461 [test] -0.463\n",
      "* [epoch] 31  [val] -0.454 [test] -0.454\n",
      "  [epoch] 32  [val] -0.464 [test] -0.462\n",
      "  [epoch] 33  [val] -0.456 [test] -0.458\n",
      "  [epoch] 34  [val] -0.457 [test] -0.457\n",
      "  [epoch] 35  [val] -0.455 [test] -0.456\n",
      "  [epoch] 36  [val] -0.456 [test] -0.457\n",
      "* [epoch] 37  [val] -0.450 [test] -0.451\n",
      "  [epoch] 38  [val] -0.459 [test] -0.463\n",
      "  [epoch] 39  [val] -0.451 [test] -0.451\n",
      "  [epoch] 40  [val] -0.457 [test] -0.459\n",
      "  [epoch] 41  [val] -0.450 [test] -0.452\n",
      "  [epoch] 42  [val] -0.469 [test] -0.469\n",
      "  [epoch] 43  [val] -0.458 [test] -0.458\n",
      "* [epoch] 44  [val] -0.448 [test] -0.449\n",
      "  [epoch] 45  [val] -0.459 [test] -0.461\n",
      "  [epoch] 46  [val] -0.453 [test] -0.458\n",
      "  [epoch] 47  [val] -0.448 [test] -0.452\n",
      "  [epoch] 48  [val] -0.474 [test] -0.474\n",
      "  [epoch] 49  [val] -0.457 [test] -0.458\n",
      "  [epoch] 50  [val] -0.450 [test] -0.454\n",
      "  [epoch] 51  [val] -0.463 [test] -0.465\n",
      "  [epoch] 52  [val] -0.455 [test] -0.457\n",
      "  [epoch] 53  [val] -0.452 [test] -0.459\n",
      "  [epoch] 54  [val] -0.451 [test] -0.456\n",
      "  [epoch] 55  [val] -0.449 [test] -0.451\n",
      "  [epoch] 56  [val] -0.453 [test] -0.453\n",
      "  [epoch] 57  [val] -0.457 [test] -0.459\n",
      "  [epoch] 58  [val] -0.457 [test] -0.456\n",
      "  [epoch] 59  [val] -0.448 [test] -0.453\n",
      "  [epoch] 60  [val] -0.463 [test] -0.463\n",
      "  [epoch] 61  [val] -0.463 [test] -0.463\n",
      "\n",
      "[Summary]\n",
      "best epoch:  44\n",
      "val score:  -0.44797421425772255\n",
      "test score: -0.4489692326732502\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1_000_000_000\n",
    "train_size = len(train_idx)\n",
    "batch_size = 256\n",
    "epoch_size = math.ceil(train_size / batch_size)\n",
    "\n",
    "epoch = -1\n",
    "metrics = {'val': -math.inf, 'test': -math.inf}\n",
    "\n",
    "\n",
    "def make_checkpoint() -> dict[str, Any]:\n",
    "    return deepcopy(\n",
    "        {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'metrics': metrics,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "best_checkpoint = make_checkpoint()\n",
    "\n",
    "# Early stopping: the training stops if the validation score\n",
    "# does not improve for more than `patience` consecutive epochs.\n",
    "patience = 16\n",
    "remaining_patience = patience\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batches = (\n",
    "        # Create one standard batch sequence.\n",
    "        torch.randperm(train_size, device=device).split(batch_size)\n",
    "        if share_training_batches\n",
    "        # Create k independent batch sequences.\n",
    "        else (\n",
    "            torch.rand((train_size, model.backbone.k), device=device)\n",
    "            .argsort(dim=0)\n",
    "            .split(batch_size, dim=0)\n",
    "        )\n",
    "    )\n",
    "    for batch_idx in batches:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(apply_model('train', batch_idx), Y_train[batch_idx])\n",
    "        if grad_scaler is None:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            grad_scaler.scale(loss).backward()  # type: ignore\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "\n",
    "    metrics = {part: evaluate(part) for part in ['val', 'test']}\n",
    "    val_score_improved = metrics['val'] > best_checkpoint['metrics']['val']\n",
    "\n",
    "    print(\n",
    "        f'{\"*\" if val_score_improved else \" \"}'\n",
    "        f' [epoch] {epoch:<3}'\n",
    "        f' [val] {metrics[\"val\"]:.3f}'\n",
    "        f' [test] {metrics[\"test\"]:.3f}'\n",
    "    )\n",
    "\n",
    "    if val_score_improved:\n",
    "        best_checkpoint = make_checkpoint()\n",
    "        remaining_patience = patience\n",
    "    else:\n",
    "        remaining_patience -= 1\n",
    "\n",
    "    if remaining_patience < 0:\n",
    "        break\n",
    "\n",
    "# To make final predictions, load the best checkpoint.\n",
    "model.load_state_dict(best_checkpoint['model'])\n",
    "\n",
    "print('\\n[Summary]')\n",
    "print(f'best epoch:  {best_checkpoint[\"epoch\"]}')\n",
    "print(f'val score:  {best_checkpoint[\"metrics\"][\"val\"]}')\n",
    "print(f'test score: {best_checkpoint[\"metrics\"][\"test\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
