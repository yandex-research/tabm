{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabM\n",
    "\n",
    "This is a standalone usage example for the TabM project.\n",
    "The easiest way to run it is [Pixi](https://pixi.sh/latest/#installation):\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/yandex-research/tabm\n",
    "cd tabm\n",
    "\n",
    "# With GPU:\n",
    "pixi run -e cuda jupyter-lab example.ipynb\n",
    "\n",
    "# Without GPU:\n",
    "pixi run jupyter-lab example.ipynb\n",
    "```\n",
    "\n",
    "For the full overview of the project, and for non-Pixi environment setups, see README in the repository:\n",
    "https://github.com/yandex-research/tabm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yandex-research/tabm\n",
    "!cd tabm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install rtdl_num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:30:20.809362Z",
     "iopub.status.busy": "2024-12-15T22:30:20.808589Z",
     "iopub.status.idle": "2024-12-15T22:30:20.822473Z",
     "shell.execute_reply": "2024-12-15T22:30:20.821298Z",
     "shell.execute_reply.started": "2024-12-15T22:30:20.809292Z"
    }
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from typing import Literal, NamedTuple\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "from tabm_reference import Model, make_parameter_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:22.698491Z",
     "iopub.status.busy": "2024-12-15T22:26:22.698205Z",
     "iopub.status.idle": "2024-12-15T22:26:22.702430Z",
     "shell.execute_reply": "2024-12-15T22:26:22.701952Z",
     "shell.execute_reply.started": "2024-12-15T22:26:22.698468Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:22.703427Z",
     "iopub.status.busy": "2024-12-15T22:26:22.703233Z",
     "iopub.status.idle": "2024-12-15T22:26:24.369629Z",
     "shell.execute_reply": "2024-12-15T22:26:24.369058Z",
     "shell.execute_reply.started": "2024-12-15T22:26:22.703406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:        CUDA\n",
      "AMP:           False (dtype: torch.bfloat16)\n",
      "torch.compile: False\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Automatic mixed precision (AMP) ##校 效孝 协孝 孝 啸校!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# torch.float16 is implemented for completeness,\n",
    "# but it was not tested in the project,\n",
    "# so torch.bfloat16 is used by default.\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float16\n",
    "    if torch.cuda.is_available()\n",
    "    else None\n",
    ")\n",
    "# Changing False to True will result in faster training on compatible hardware.\n",
    "amp_enabled = False and amp_dtype is not None\n",
    "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
    "\n",
    "# torch.compile\n",
    "compile_model = False\n",
    "\n",
    "# fmt: off\n",
    "print(\n",
    "    f'Device:        {device.type.upper()}'\n",
    "    f'\\nAMP:           {amp_enabled} (dtype: {amp_dtype})'\n",
    "    f'\\ntorch.compile: {compile_model}'\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Dataset is \"Regression with an Insurance Dataset\"\n",
    "\n",
    "https://www.kaggle.com/competitions/playground-series-s4e12/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T16:14:42.574928Z",
     "iopub.status.busy": "2024-12-15T16:14:42.574608Z",
     "iopub.status.idle": "2024-12-15T16:14:48.784632Z",
     "shell.execute_reply": "2024-12-15T16:14:48.783365Z",
     "shell.execute_reply.started": "2024-12-15T16:14:42.574898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  185M  100  185M    0     0  65.3M      0  0:00:02  0:00:02 --:--:-- 82.5M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  119M  100  119M    0     0  64.5M      0  0:00:01  0:00:01 --:--:-- 70.3M\n"
     ]
    }
   ],
   "source": [
    "! curl -L $(yadisk-direct https://disk.yandex.ru/d/YbkU_KfAqGtdXg) -o insurance_dataset_train.csv\n",
    "! curl -L $(yadisk-direct https://disk.yandex.ru/d/_gs3p3yvp0TNRg) -o insurance_dataset_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:24.372983Z",
     "iopub.status.busy": "2024-12-15T22:26:24.372768Z",
     "iopub.status.idle": "2024-12-15T22:26:28.797704Z",
     "shell.execute_reply": "2024-12-15T22:26:28.796887Z",
     "shell.execute_reply.started": "2024-12-15T22:26:24.372948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upload train and test parts\n",
    "train_df = pd.read_csv(\"./insurance_dataset_train.csv\")\n",
    "test_df = pd.read_csv(\"./insurance_dataset_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define lists with numerical and categorical column names.\n",
    "\n",
    "Also we exclude `id` and `Policy Start Date` cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:28.798935Z",
     "iopub.status.busy": "2024-12-15T22:26:28.798708Z",
     "iopub.status.idle": "2024-12-15T22:26:28.802701Z",
     "shell.execute_reply": "2024-12-15T22:26:28.802222Z",
     "shell.execute_reply.started": "2024-12-15T22:26:28.798911Z"
    }
   },
   "outputs": [],
   "source": [
    "target_col = \"Premium Amount\"\n",
    "\n",
    "num_cols = ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', \n",
    "            'Previous Claims', 'Vehicle Age', 'Credit Score', 'Insurance Duration']\n",
    "cat_cols = ['Gender', 'Marital Status', 'Education Level', 'Occupation', 'Location',\n",
    "            'Policy Type', 'Customer Feedback', 'Smoking Status', 'Exercise Frequency',\n",
    "            'Property Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train part on train and val, test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:28.803804Z",
     "iopub.status.busy": "2024-12-15T22:26:28.803542Z",
     "iopub.status.idle": "2024-12-15T22:26:30.425082Z",
     "shell.execute_reply": "2024-12-15T22:26:30.424252Z",
     "shell.execute_reply.started": "2024-12-15T22:26:28.803783Z"
    }
   },
   "outputs": [],
   "source": [
    "# collect number of unique values for categorical columns. Will be needed in future.\n",
    "cat_cardinalities = train_df[cat_cols].nunique(dropna=False).to_list()\n",
    "\n",
    "all_idx = np.arange(len(train_df))\n",
    "# Select the test part\n",
    "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
    "    all_idx, train_size=0.8\n",
    ")\n",
    "\n",
    "# Fill dict with all parts we have\n",
    "_train_df = train_df.iloc[train_idx]\n",
    "_val_df = train_df.iloc[val_idx]\n",
    "\n",
    "data = {\n",
    "    'train': {\n",
    "        'x_num': _train_df[num_cols],\n",
    "        'x_cat': _train_df[cat_cols],\n",
    "        'y': _train_df[target_col].to_numpy().astype(np.float32)\n",
    "    },\n",
    "    'val': {\n",
    "        'x_num': _val_df[num_cols],\n",
    "        'x_cat': _val_df[cat_cols],\n",
    "        'y': _val_df[target_col].to_numpy().astype(np.float32)\n",
    "    }\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    \"x_num\": test_df[num_cols],\n",
    "    \"x_cat\": test_df[cat_cols],\n",
    "    'id': test_df['id'].to_numpy()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:30.426244Z",
     "iopub.status.busy": "2024-12-15T22:26:30.425894Z",
     "iopub.status.idle": "2024-12-15T22:26:30.433008Z",
     "shell.execute_reply": "2024-12-15T22:26:30.432500Z",
     "shell.execute_reply.started": "2024-12-15T22:26:30.426222Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define processing pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:30.434604Z",
     "iopub.status.busy": "2024-12-15T22:26:30.434341Z",
     "iopub.status.idle": "2024-12-15T22:26:30.438067Z",
     "shell.execute_reply": "2024-12-15T22:26:30.437539Z",
     "shell.execute_reply.started": "2024-12-15T22:26:30.434583Z"
    }
   },
   "outputs": [],
   "source": [
    "n_quantiles = max(min(len(train_idx) // 30, 1000), 10)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", QuantileTransformer(n_quantiles=n_quantiles, \n",
    "                                   output_distribution='normal',\n",
    "                                   subsample=10**9))\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply processing operations to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:30.438909Z",
     "iopub.status.busy": "2024-12-15T22:26:30.438727Z",
     "iopub.status.idle": "2024-12-15T22:26:37.889857Z",
     "shell.execute_reply": "2024-12-15T22:26:37.889140Z",
     "shell.execute_reply.started": "2024-12-15T22:26:30.438888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`x_num` dtype: <class 'numpy.ndarray'>\n",
      "`x_cat` dtype: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
    "noise = (\n",
    "    np.random.default_rng(0)\n",
    "    .normal(0.0, 1e-5, data['train']['x_num'].shape)\n",
    ")\n",
    "\n",
    "# Fit pipelines on train part\n",
    "num_processor = num_pipeline.fit(data['train']['x_num'] + noise)\n",
    "cat_processor = cat_pipeline.fit(data['train']['x_cat'])\n",
    "\n",
    "# Apply the processing to all parts. When processor is applied, each dataframe converts to np.ndarray\n",
    "for part in data:\n",
    "    data[part]['x_num'] = num_processor.transform(data[part]['x_num']).astype(np.float32)\n",
    "    data[part]['x_cat'] = cat_processor.transform(data[part]['x_cat']).astype(np.int64)\n",
    "\n",
    "print(f\"`x_num` dtype: {type(data[\"train\"]['x_num'])}\\n`x_cat` dtype: {type(data[\"train\"]['x_cat'])}\")\n",
    "\n",
    "# Apply processing to test data as well\n",
    "\n",
    "test_data[\"x_num\"] = num_processor.transform(test_data['x_num']).astype(np.float32)\n",
    "test_data[\"x_cat\"] = cat_processor.transform(test_data['x_cat']).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert every part of the dataset to `torch.tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:26:37.891108Z",
     "iopub.status.busy": "2024-12-15T22:26:37.890884Z",
     "iopub.status.idle": "2024-12-15T22:26:41.289977Z",
     "shell.execute_reply": "2024-12-15T22:26:41.289243Z",
     "shell.execute_reply.started": "2024-12-15T22:26:37.891085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "data_torch = {\n",
    "    part: {k: torch.as_tensor(v, device=device) for k, v in data[part].items()}\n",
    "    for part in data\n",
    "}\n",
    "\n",
    "test_data_torch = {\n",
    "    \"x_num\": torch.as_tensor(test_data[\"x_num\"], device=device),\n",
    "    \"x_cat\": torch.as_tensor(test_data[\"x_cat\"], device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:55:02.506568Z",
     "iopub.status.busy": "2024-12-15T22:55:02.506216Z",
     "iopub.status.idle": "2024-12-15T22:55:02.546361Z",
     "shell.execute_reply": "2024-12-15T22:55:02.544947Z",
     "shell.execute_reply.started": "2024-12-15T22:55:02.506534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose one of the two configurations below.\n",
    "\n",
    "# TabM\n",
    "arch_type = 'tabm'\n",
    "bins = None\n",
    "\n",
    "# TabM-mini with the piecewise-linear embeddings.\n",
    "# arch_type = 'tabm-mini'\n",
    "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_num'])\n",
    "\n",
    "model = Model(\n",
    "    n_num_features=len(num_cols),\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    n_classes=None,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 16,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=32,\n",
    ").to(device)\n",
    "\n",
    "if compile_model:\n",
    "    # NOTE\n",
    "    # `torch.compile` is intentionally called without the `mode` argument\n",
    "    # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n",
    "    model = torch.compile(model)\n",
    "    evaluation_mode = torch.no_grad\n",
    "else:\n",
    "    evaluation_mode = torch.inference_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:55:05.279728Z",
     "iopub.status.busy": "2024-12-15T22:55:05.279288Z",
     "iopub.status.idle": "2024-12-15T22:55:05.288377Z",
     "shell.execute_reply": "2024-12-15T22:55:05.286916Z",
     "shell.execute_reply.started": "2024-12-15T22:55:05.279681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (cat_module): OneHotEncoding0d()\n",
       "  (backbone): MLP(\n",
       "    (blocks): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): LinearEfficientEnsemble()\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): NLinear()\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:31:43.778579Z",
     "iopub.status.busy": "2024-12-15T22:31:43.777580Z",
     "iopub.status.idle": "2024-12-15T22:31:43.922162Z",
     "shell.execute_reply": "2024-12-15T22:31:43.920892Z",
     "shell.execute_reply.started": "2024-12-15T22:31:43.778512Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcat_cardinalities\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/anton/shad/lection_5_tabm/tabm/tabm_reference.py:583\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_num, x_cat)\u001b[0m\n\u001b[1;32m    581\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(x_num \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_module(x_num))\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_cat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1, len(num_cols) + sum(cat_cardinalities), ), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Loss in kaggle competition is RMSLE (Root Mean Squared Logarithmic Error) let's use it as validation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:55:08.720579Z",
     "iopub.status.busy": "2024-12-15T22:55:08.720030Z",
     "iopub.status.idle": "2024-12-15T22:55:08.728769Z",
     "shell.execute_reply": "2024-12-15T22:55:08.727388Z",
     "shell.execute_reply.started": "2024-12-15T22:55:08.720513Z"
    }
   },
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        return torch.sqrt(self.mse(torch.log(input + 1), torch.log(target + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define class for training and evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:55:09.318159Z",
     "iopub.status.busy": "2024-12-15T22:55:09.317107Z",
     "iopub.status.idle": "2024-12-15T22:55:09.354255Z",
     "shell.execute_reply": "2024-12-15T22:55:09.352662Z",
     "shell.execute_reply.started": "2024-12-15T22:55:09.318095Z"
    }
   },
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    \"\"\"Runner for train/evaludate and predict using provided model.\"\"\"\n",
    "    def __init__(self, model, optimizer, loss, device, eval_metric=None, grad_scaler=None, epoch_bar=False):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.device = device\n",
    "        self.eval_metric = loss if eval_metric is None else eval_metric\n",
    "        self.grad_scaler = grad_scaler\n",
    "        self.epoch_bar = epoch_bar\n",
    "\n",
    "        self._train_mean = None\n",
    "        self._train_std = None\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        # TabM produces k predictions per object. Each of them must be trained separately.\n",
    "        # (regression)     y_pred.shape == (batch_size, k)\n",
    "        # (classification) y_pred.shape == (batch_size, k, n_classes)\n",
    "        k = y_pred.shape[-1]\n",
    "        return self.loss(y_pred.flatten(0, 1), y_true.repeat_interleave(k))\n",
    "\n",
    "    def forward(self, batch_num, batch_cat=None):\n",
    "        return (\n",
    "            self.model(batch_num, batch_cat)\n",
    "            .squeeze(-1)\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "    def _train_step(self, y_pred, y_true, normalize_target):\n",
    "        if normalize_target :\n",
    "            y_true = (y_true - self._train_mean) / self._train_std\n",
    "        loss = self.compute_loss(y_pred, y_true)\n",
    "        if self.grad_scaler is None:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        else:\n",
    "            self.grad_scaler.scale(loss).backward()  # type: ignore\n",
    "            self.grad_scaler.step(optimizer)\n",
    "            self.grad_scaler.update()\n",
    "        self.optimizer.zero_grad()\n",
    "        return loss.detach().cpu().numpy().mean()\n",
    "    \n",
    "    def _eval_step(self, y_pred, y_true):\n",
    "        # Compute the mean of the k predictions\n",
    "        y_pred = y_pred.mean(1) * self._train_std + self._train_mean\n",
    "        # Avoid computing gradients if train_phase is False\n",
    "        with torch.set_grad_enabled(False):\n",
    "            metric = self.eval_metric(y_pred, y_true)\n",
    "        return metric.detach().cpu().numpy()\n",
    "\n",
    "    def _run_epoch(self, data, batch_indices, train_phase=True, normalize_target=True):\n",
    "        self.model.train(train_phase)\n",
    "        epoch_history = []\n",
    "        for batch_idx in tqdm(batch_indices, disable=not self.epoch_bar, leave=False):\n",
    "            batch_num = data['x_num'][batch_idx]\n",
    "            batch_cat = data['x_cat'][batch_idx]\n",
    "            y_true = data['y'][batch_idx]\n",
    "            y_pred = self.forward(batch_num, batch_cat)\n",
    "\n",
    "            if train_phase:\n",
    "                res = self._train_step(y_pred, y_true, normalize_target=normalize_target)\n",
    "            else:\n",
    "                res = self._eval_step(y_pred, y_true)\n",
    "            epoch_history.append(res)\n",
    "        return np.mean(epoch_history)\n",
    "\n",
    "    def train(self, train_data, val_data, batch_size=256, n_epochs=1000, patience=16,\n",
    "              eval_batch_size=None, normalize_target=True, force_collect_stats=False):\n",
    "        best = {\n",
    "            'val': math.inf,\n",
    "            'test': math.inf,\n",
    "            'epoch': -1,\n",
    "        }\n",
    "\n",
    "        # Important!\n",
    "        # For regression tasks it is highly recommended to standardize the training labels.\n",
    "        self._train_mean = (train_data['y'].mean() \n",
    "                            if self._train_mean is None or force_collect_stats \n",
    "                            else self._train_mean)\n",
    "        self._train_std = (train_data['y'].std() \n",
    "                           if self._train_std is None or force_collect_stats \n",
    "                           else self._train_std)\n",
    "\n",
    "        # Early stopping: the training stops when\n",
    "        # there are more than `patience` consequtive bad updates.\n",
    "        remaining_patience = patience\n",
    "        eval_batch_size = batch_size if eval_batch_size is None else eval_batch_size\n",
    "        for epoch in tqdm(range(n_epochs), total=n_epochs):\n",
    "            batch_indices = torch.randperm(len(train_data['y']), device=device).split(batch_size)\n",
    "            train_score = self._run_epoch(train_data, batch_indices, train_phase=True,\n",
    "                                          normalize_target=normalize_target)\n",
    "            val_score = self.evaluate(val_data, eval_batch_size)\n",
    "\n",
    "            msg = f\"Epoch: {epoch} (train) {train_score:.4f} (val) {val_score:.4f}\"\n",
    "        \n",
    "            if val_score < best['val']:\n",
    "                best = {'val': val_score, 'epoch': epoch}\n",
    "                remaining_patience = patience\n",
    "                print(msg + \"\\t New best epoch! \")\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "                print(msg)\n",
    "        \n",
    "            if remaining_patience < 0:\n",
    "                break\n",
    "        print(f\"Training is complete.\\n Best model: epoch {best['epoch']} metric {best['val']}\")\n",
    "\n",
    "    @evaluation_mode\n",
    "    def evaluate(self, data, batch_size=256):\n",
    "        batch_indices = torch.randperm(len(data['y']), device=device).split(batch_size)\n",
    "        return self._run_epoch(data, batch_indices, train_phase=False)\n",
    "\n",
    "    @evaluation_mode\n",
    "    def predict(self, data, batch_size=256):\n",
    "        batch_indices = torch.arange(len(data['x_num']), device=device).split(batch_size)\n",
    "        preds = []\n",
    "        for batch_idx in tqdm(batch_indices, disable=not self.epoch_bar, leave=False):\n",
    "            batch_num = data['x_num'][batch_idx]\n",
    "            batch_cat = data['x_cat'][batch_idx]\n",
    "            y_pred = self.forward(batch_num, batch_cat)\n",
    "            y_pred = y_pred.mean(1) * self._train_std + self._train_mean\n",
    "            preds.extend(y_pred.detach().cpu().numpy())\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create runner instance and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:55:09.993117Z",
     "iopub.status.busy": "2024-12-15T22:55:09.992554Z",
     "iopub.status.idle": "2024-12-15T22:55:10.003149Z",
     "shell.execute_reply": "2024-12-15T22:55:10.001723Z",
     "shell.execute_reply.started": "2024-12-15T22:55:09.993054Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=2e-3, weight_decay=3e-4)\n",
    "loss = F.mse_loss\n",
    "eval_metric = RMSLELoss()\n",
    "\n",
    "runner = Runner(model, optimizer, loss, device=device, eval_metric=eval_metric, grad_scaler=grad_scaler, epoch_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:55:11.069809Z",
     "iopub.status.busy": "2024-12-15T22:55:11.069254Z",
     "iopub.status.idle": "2024-12-15T22:58:53.571176Z",
     "shell.execute_reply": "2024-12-15T22:58:53.570203Z",
     "shell.execute_reply.started": "2024-12-15T22:55:11.069746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8c2ac352814d5d8d01d1ae2d0835d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 (train) 0.9901 (val) 1.1629\t New best epoch! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (train) 0.9803 (val) 1.1563\t New best epoch! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (train) 0.9765 (val) 1.1571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 (train) 0.9741 (val) 1.1554\t New best epoch! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 (train) 0.9716 (val) 1.1548\t New best epoch! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 (train) 0.9690 (val) 1.1497\t New best epoch! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 (train) 0.9653 (val) 1.1550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 (train) 0.9620 (val) 1.1512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 (train) 0.9596 (val) 1.1475\t New best epoch! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 (train) 0.9585 (val) 1.1457\t New best epoch! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 (train) 0.9568 (val) 1.1398\t New best epoch! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 (train) 0.9558 (val) 1.1478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 (train) 0.9546 (val) 1.1399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 (train) 0.9532 (val) 1.1415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 (train) 0.9514 (val) 1.1405\n",
      "Training is complete.\n",
      " Best model: epoch 10 metric 1.1398487091064453\n"
     ]
    }
   ],
   "source": [
    "runner.train(data_torch['train'], data_torch['val'], batch_size=10048, n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to improve the result??\n",
    "* Play with network params\n",
    "* Try to use bins, tabm_mini\n",
    "* Other ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about standard approaches? Let's try catboost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:41:09.985542Z",
     "iopub.status.busy": "2024-12-15T22:41:09.984921Z",
     "iopub.status.idle": "2024-12-15T22:41:09.991532Z",
     "shell.execute_reply": "2024-12-15T22:41:09.990270Z",
     "shell.execute_reply.started": "2024-12-15T22:41:09.985476Z"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:47:45.870558Z",
     "iopub.status.busy": "2024-12-15T22:47:45.870258Z",
     "iopub.status.idle": "2024-12-15T22:53:03.465254Z",
     "shell.execute_reply": "2024-12-15T22:53:03.463857Z",
     "shell.execute_reply.started": "2024-12-15T22:47:45.870528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.125513\n",
      "0:\tlearn: 861.9131440\ttotal: 433ms\tremaining: 7m 12s\n",
      "50:\tlearn: 844.2595204\ttotal: 16.1s\tremaining: 4m 58s\n",
      "100:\tlearn: 842.8552496\ttotal: 29.4s\tremaining: 4m 22s\n",
      "150:\tlearn: 841.9049956\ttotal: 44s\tremaining: 4m 7s\n",
      "200:\tlearn: 841.1426827\ttotal: 59.5s\tremaining: 3m 56s\n",
      "250:\tlearn: 840.5975103\ttotal: 1m 14s\tremaining: 3m 42s\n",
      "300:\tlearn: 840.2264102\ttotal: 1m 29s\tremaining: 3m 28s\n",
      "350:\tlearn: 839.7998081\ttotal: 1m 46s\tremaining: 3m 16s\n",
      "400:\tlearn: 839.4441462\ttotal: 2m 2s\tremaining: 3m 2s\n",
      "450:\tlearn: 839.1979030\ttotal: 2m 18s\tremaining: 2m 48s\n",
      "500:\tlearn: 838.9704194\ttotal: 2m 34s\tremaining: 2m 33s\n",
      "550:\tlearn: 838.7191916\ttotal: 2m 50s\tremaining: 2m 18s\n",
      "600:\tlearn: 838.4715651\ttotal: 3m 6s\tremaining: 2m 3s\n",
      "650:\tlearn: 838.2279777\ttotal: 3m 22s\tremaining: 1m 48s\n",
      "700:\tlearn: 837.9883450\ttotal: 3m 38s\tremaining: 1m 33s\n",
      "750:\tlearn: 837.7801345\ttotal: 3m 54s\tremaining: 1m 17s\n",
      "800:\tlearn: 837.5747252\ttotal: 4m 10s\tremaining: 1m 2s\n",
      "850:\tlearn: 837.3360883\ttotal: 4m 26s\tremaining: 46.7s\n",
      "900:\tlearn: 837.1117564\ttotal: 4m 42s\tremaining: 31.1s\n",
      "950:\tlearn: 836.8848780\ttotal: 4m 58s\tremaining: 15.4s\n",
      "999:\tlearn: 836.6872657\ttotal: 5m 14s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7fd5f677e570>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "X_cb = train_df[cat_cols + num_cols].fillna(0)\n",
    "y_cb = train_df[target_col]\n",
    "catboost_model.fit(X_cb, y_cb, cat_features=cat_cols, verbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:53:05.866317Z",
     "iopub.status.busy": "2024-12-15T22:53:05.865935Z",
     "iopub.status.idle": "2024-12-15T22:53:07.587161Z",
     "shell.execute_reply": "2024-12-15T22:53:07.585720Z",
     "shell.execute_reply.started": "2024-12-15T22:53:05.866282Z"
    }
   },
   "outputs": [],
   "source": [
    "catboost_preds = catboost_model.predict(test_df[cat_cols + num_cols].fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:35:29.800479Z",
     "iopub.status.busy": "2024-12-15T22:35:29.800201Z",
     "iopub.status.idle": "2024-12-15T22:35:29.805444Z",
     "shell.execute_reply": "2024-12-15T22:35:29.804617Z",
     "shell.execute_reply.started": "2024-12-15T22:35:29.800457Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_submission(preds, ids, path=None):\n",
    "    result = pd.DataFrame({\"id\": ids, \"Premium Amount\": preds})\n",
    "    if path is None:\n",
    "        return result\n",
    "    result.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:35:20.676592Z",
     "iopub.status.busy": "2024-12-15T22:35:20.675619Z",
     "iopub.status.idle": "2024-12-15T22:35:24.627000Z",
     "shell.execute_reply": "2024-12-15T22:35:24.626118Z",
     "shell.execute_reply.started": "2024-12-15T22:35:20.676526Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = runner.predict(test_data_torch, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:35:30.722126Z",
     "iopub.status.busy": "2024-12-15T22:35:30.721718Z",
     "iopub.status.idle": "2024-12-15T22:35:32.079075Z",
     "shell.execute_reply": "2024-12-15T22:35:32.077729Z",
     "shell.execute_reply.started": "2024-12-15T22:35:30.722082Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submission(preds, test_data[\"id\"], \"simple_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T22:53:44.444726Z",
     "iopub.status.busy": "2024-12-15T22:53:44.444115Z",
     "iopub.status.idle": "2024-12-15T22:53:45.989100Z",
     "shell.execute_reply": "2024-12-15T22:53:45.988168Z",
     "shell.execute_reply.started": "2024-12-15T22:53:44.444659Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submission(catboost_preds, test_df[\"id\"], \"simple_catboost_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit using API\n",
    "\n",
    "You can submit your solution directly from colab!\n",
    "\n",
    "\n",
    "```python\n",
    "kaggle competitions submit -c playground-series-s4e12 -f submission.csv -m \"Message\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
